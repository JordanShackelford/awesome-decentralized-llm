# awesome-decentralized-llm

This is a collection of resources that I will at some point clean up and organize.

### Repositories

- [GPT4All](https://github.com/nomic-ai/gpt4all) -
  Demo, data and code to train an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMa.
  (2023-03-28, Nomic AI)
  
- [alpaca.cpp](https://github.com/antimatter15/alpaca.cpp) -
  Locally run an Instruction-Tuned Chat-Style LLM
  (2023-03-16, Kevin Kwok)

- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) -
  Code and documentation to train Stanford's Alpaca models, and generate the data.
  (2023-03-13, Stanford CRFM)

- [llama.cpp](https://github.com/ggerganov/llama.cpp) -
  Port of Facebook's LLaMA model in C/C++. 
  (2023-03-10, Georgi Gerganov)

- [whisper.cpp](https://github.com/ggerganov/whisper.cpp) -
  Port of OpenAI's Whisper model in C/C++.
  (2022-12-07, Georgi Gerganov)


### Resources

- [Bringing Whisper and LLaMA to the masses](https://changelog.com/podcast/532)
  (2022-03-15, The Changelog & Georgi Gerganov, Podcast Episode)
  
- [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
  (2023-03-13, Stanford CRFM, Project Homepage)

- [Large language models are having their Stable Diffusion moment](https://simonwillison.net/2023/Mar/11/llama/)
  (2023-03-10, Simon Willison, Blog)

- [Running LLaMA 7B and 13B on a 64GB M2 MacBook Pro with llama.cpp](https://til.simonwillison.net/llms/llama-7b-m2)
  (2023-03-10, Simon Willison, Blog/Today I Learned)
  
