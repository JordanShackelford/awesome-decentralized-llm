# awesome-decentralized-llm

This is a collection of resources that I will at some point clean up and organize.

I am trying to track details like whether a model falls under OpenAI's Terms of Service (ToS), let me know if there are other similar issues.

### Repositories

- [GPT4All](https://github.com/nomic-ai/gpt4all) -
  LLM trained with ~800k GPT-3.5-Turbo Generations based on LLaMa. (OpenAI ToS)
  (2023-03-28, Nomic AI)
  
- [bloomz.cpp](https://github.com/NouamaneTazi/bloomz.cpp)
  Inference of HuggingFace's BLOOM-like models in pure C/C++.
  (2023-03-16, Nouamane Tazi)
  
- [alpaca.cpp](https://github.com/antimatter15/alpaca.cpp) -
  Locally run an Instruction-Tuned Chat-Style LLM
  (2023-03-16, Kevin Kwok)

- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) -
  Code and documentation to train Stanford's Alpaca models, and generate the data. (OpenAI ToS)
  (2023-03-13, Stanford CRFM)

- [llama.cpp](https://github.com/ggerganov/llama.cpp) -
  Port of Facebook's LLaMA model in C/C++. 
  (2023-03-10, Georgi Gerganov)

- [ChatRWKV](https://github.com/BlinkDL/ChatRWKV) -
  ChatRWKV is like ChatGPT but powered by RWKV (100% RNN) language model, and open source.
  ()
  
- [RWKV-LM](https://github.com/BlinkDL/RWKV-LM) -
  RNN with Transformer-level LLM performance. Combines best of RNN and transformer: fast inference, saves VRAM, fast training.
  (2022?, PENG Bo)


### Spaces, Models & Datasets

- [Cerebras-GPT 7 Models](https://huggingface.co/cerebras)
  (2023-03-28, Huggingface, Cerebras)
 
- [Alpaca Dataset](https://huggingface.co/datasets/tatsu-lab/alpaca)
  (2023-03-13, Huggingface, Tatsu-Lab)
  
- [Alpaca Model Search](https://huggingface.co/models?sort=downloads&search=alpaca)
  (Huggingface)
  

### Resources

- [Cerebras-GPT: Family of Open, Compute-efficient, LLMs](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/)
  Seven GPT-3 models from 111M - 13B parameters using the Chinchilla formula. Set new benchmarks for accuracy and compute efficiency.
  (2022-03-28, Cerebras, Blog Post)

- [Bringing Whisper and LLaMA to the masses](https://changelog.com/podcast/532)
  (2022-03-15, The Changelog & Georgi Gerganov, Podcast Episode)
  
- [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
  (2023-03-13, Stanford CRFM, Project Homepage)

- [Large language models are having their Stable Diffusion moment](https://simonwillison.net/2023/Mar/11/llama/)
  (2023-03-10, Simon Willison, Blog Post)

- [Running LLaMA 7B and 13B on a 64GB M2 MacBook Pro with llama.cpp](https://til.simonwillison.net/llms/llama-7b-m2)
  (2023-03-10, Simon Willison, Blog/Today I Learned)
  
- [Introducing LLaMA: A foundational, 65-billion-parameter large language model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
  (2023-02-24, Meta AI)
